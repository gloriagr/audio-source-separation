#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 3cm
\rightmargin 3cm
\bottommargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title

\series bold
DEEP LEARNING ON COMPUTATIONAL ACCELERATORS 236605
\begin_inset Newline newline
\end_inset

 
\series default

\begin_inset Newline newline
\end_inset


\series bold
PAPER IMPLEMENTATION
\series default

\begin_inset Newline newline
\end_inset


\series bold

\begin_inset Newline newline
\end_inset

Monoaural Audio Source Separation Using Deep Convolutional Neural Networks
\series default

\begin_inset Newline newline
\end_inset

by P.
 Chandna, M.
 Miron, J.
 Janer, E.
 GÃ³mez
\end_layout

\begin_layout Date
Technion IIT, spring 2019
\end_layout

\begin_layout Author
Danny Priymak 307003434
\begin_inset Formula $\qquad\qquad$
\end_inset

Gloria Groysman 318372729
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Source separation is the separation of a set of source signals from a set
 of mixed signals, without the aid of information (or with very little informati
on) about the source signals or the mixing process.
 It is most commonly applied in digital signal processing and involves the
 analysis of mixtures of signals; the objective is to recover the original
 component signals from a mixture signal.
 Audio source separation has drawn the attention of researchers, with approaches
 varying from using timbre models 
\begin_inset CommandInset citation
LatexCommand cite
key "key-2"
literal "false"

\end_inset

, to those exploiting the repetitive nature of music 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"
literal "false"

\end_inset

.
 While being an interesting problem in itself, the separation of sources
 from a mixture can serve as a intermediary step for other tasks such as
 automatic speech recognition and fundamental frequency estimation.
 Some applications, such as speech enhancement for cochlear implant users,
 require low-latency processing.
 The original paper we chose to focus on, by Chandna et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"
literal "false"

\end_inset

 has tackled the audio signal source separation case, and has introduced
 a low-latency monaural source separation framework using a convolutional
 neural network (CNN).
 The authors used a CNN to estimate time-frequency soft masks, generated
 per-instrument, which in turn are applied to musical tracks in order to
 isolate the corresponding instrument's estimated, isolated audio.
 The training, as well as an evaluation of the neural network's performance
 was performed over a dataset containing fully mixed audio music tracks
 of four fixed instruments: vocals, drums, and bass, as well as a fourth
 instrument category representing all other remaining instruments, which
 vary from track to track.
 The authors' proposed architecture has been compared to a Multilayer Perceptron
 (MLP) architecture, and achieved on-par results in conjunction with a significa
nt improvement in processing time.
 Our work intends to implement this architecture and provide possible enhancemen
ts.
\end_layout

\begin_layout Section
Related Work
\end_layout

\begin_layout Standard
Several approaches have been proposed for the solution of this problem but
 development is currently still very much in progress.
 A more successful classical approach that does not involve machine learning
 concepts is non-negative matrix factorization (NMF).
 Additional popular approaches are principal components analysis and independent
 component analysis, which work well when there are no delays or echoes
 present; that is, the problem is simplified a great deal.
 
\end_layout

\begin_layout Standard
Approaches directly using deep neural networks for separation have been
 proposed as well.
 Nugraha et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-4"
literal "false"

\end_inset

 adapt deep neural networks for multichannel source separation, using both
 phase and magnitude information.
 As for monaural source separation, Huang et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-5"
literal "false"

\end_inset

 propose a method that utilizes a deep neural network, taking a single frame
 of the magnitude spectrogram of a mixture as an input feature to learn
 single-frame timbre features for each source.
 Temporal evolution is then modeled using a recurrent layer.
 Uhlich et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-6"
literal "false"

\end_inset

 propose another method which takes multiple frames of the magnitude spectrogram
 of a mixture as input and consists of only fully connected layers.
 This method models timbre features across multiple time frames.
 While these approaches work well, they do not exploit completely local
 time-frequency features.
 Instead, they rely on global features across the entire frequency spectrum,
 over a longer period of time.
 
\end_layout

\begin_layout Section
Method
\end_layout

\begin_layout Standard
Convolutional neural networks (CNNs), take advantage of small scale features
 present in data.
 CNNs require less memory and resources than regular fully connected neural
 networks, allowing for a faster, more efficient model.
 CNNs have proved to be successful in image processing for tasks such as
 image super-resolution and semantic segmentation of images.
 In the image processing field, CNNs take as input a two-dimensional vector
 of pixel intensities across the spatial dimension and exploit the local
 spatial correlation among input neurons to learn localized features.
 A similar two-dimensional representation is used in our model for audio
 mixtures, using the Short-Time Fourier Transform (STFT), which has frequency
 and time dimensions.
 Unlike 2D images, the STFT does not have symmetry across both axes, but
 a local symmetry can be found along each single axes.
 Therefore, the filters used in CNNs need to be adapted to the STFT representati
on of audio.
\end_layout

\begin_layout Standard
Figure 1 shows the block diagram for the proposed source separation framework.
 The STFT is computed on a segment of time context 
\begin_inset Formula $T$
\end_inset

 of the mixture audio.
 The resulting magnitude spectrogram is then passed through the network,
 which outputs an estimate for each of the separated sources.
 The estimate is used to compute time-frequency soft masks, which are applied
 to the magnitude spectrogram of the mixture to compute final magnitude
 estimates for the separated sources.
 These estimates, along with the phase of the mixture, are used to obtain
 the audio signals corresponding to the separated sources.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename data_flow.png
	lyxscale 20
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Data Flow
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Architecture
\end_layout

\begin_layout Standard
The authors use a CNN which functions as a variation of an autoencoder architect
ure.
 The network is able to learn an end-to-end model for the separated sources
 by finding a compressed representation for the training data.
 The model proposed is shown in Figure 2.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig2.png
	lyxscale 30
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Network Architecture for Source Separation, Using Vertical And Hori-zontal
 Convolutions, Showing The Encoding And Decoding Stages
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
It uses a CNN with two stages: a convolution or encoding stage and the inverse
 operation, the deconvolution or decoding stage.
 The stages are explained in further details below.
\end_layout

\begin_layout Paragraph*
Encoding Stage.
\end_layout

\begin_layout Standard
This part of the network consists of two convolution layers and a fully
 connected dense layer, which acts as a bottleneck to compress information.
\end_layout

\begin_layout Enumerate

\series bold
Vertical convolution layer.

\series default
 This convolution layer has a shape of 
\begin_inset Formula $(t_{1},f_{1})$
\end_inset

 , spanning across a 
\begin_inset Formula $t_{1}$
\end_inset

 time frame and taking into account 
\begin_inset Formula $f_{1}$
\end_inset

 frequency bins.
 This layer tries to capture local timbre information, allowing the model
 to learn timbre features, similar to the approach used in NMF algorithms
 for source separation.
 These features are shared among the sources to be separated, contrary to
 the NMF approach, where specific basis and activation gains are derived
 for each source.
 Therefore, the timbre features learned by this layer need to be robust
 enough to separate the required source across songs of different genres,
 where the type of instruments and singers might vary.
 
\begin_inset Formula $N_{1}$
\end_inset

 filters were used in this layer.
\end_layout

\begin_layout Enumerate

\series bold
Horizontal convolution layer.

\series default
 This layer models temporal evolution for different instruments from the
 features learned in the vertical convolution layer.
 This is particularly useful for modeling time-frequency characteristics
 of the different instruments present in the sources to be separated.
 The filter shape of this layer is 
\begin_inset Formula $(t_{2},f_{2})$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset

 filters were used.
\end_layout

\begin_layout Enumerate

\series bold
Fully connected layer.

\series default
 The output of the horizontal convolution layer is connected to a fully
 connected Rectified Linear Unit (ReLU) layer which acts as a bottleneck,
 achieving dimensional reduction.
 This layer consists of a non-linear combination of the features learned
 from the previous layers, with a ReLU non-linearity.
 The layer is chosen to have fewer elements to reduce the total number of
 network parameters and to ensure that the networkis able to produce a robust
 representation of the input data.
 The number of nodes in this layer is represented as 
\begin_inset Formula $NN$
\end_inset

.
 
\end_layout

\begin_layout Paragraph*
Decoding Stage.
\end_layout

\begin_layout Standard
The output of the first fully connected layer is passed to another fully
 connected layer, with a ReLU non-linearity and the same size as the output
 of the second convolution layer.
 Thereafter, this layer is reshaped to the same dimensions as the horizontal
 convolution layer and passed through successive deconvolution layers, the
 inverse operations to the convolution stage.
\end_layout

\begin_layout Subsection
Time-frequency masking
\end_layout

\begin_layout Standard
The authors wished integrate the computation of a softmask for each of the
 sources into the network.
 From the output of the network 
\begin_inset Formula $\hat{y}_{n}\left(f\right)$
\end_inset

, a soft mask 
\begin_inset Formula $m_{n}\left(f\right)$
\end_inset

 can be computed as follows
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
m_{n}\left(f\right)=\frac{\left|\hat{y}_{n}\left(f\right)\right|}{\sum_{n=1}^{N}\left|\hat{y}_{n}\left(f\right)\right|}
\]

\end_inset

where 
\begin_inset Formula $\hat{y}_{n}\left(f\right)$
\end_inset

 represents the output of the network for the 
\begin_inset Formula $n$
\end_inset

-th source and 
\begin_inset Formula $N$
\end_inset

 is the total number of sources to be estimated.
 The estimated mask is then applied to the input mixture signal to estimate
 the sources 
\begin_inset Formula $\widetilde{y}_{n}$
\end_inset


\begin_inset Formula 
\[
\widetilde{y}_{n}\left(f\right)=m_{n}\left(f\right)x\left(f\right)
\]

\end_inset

where 
\begin_inset Formula $x\left(f\right)$
\end_inset

 is the spectrogram of the input mixture signal.
\end_layout

\begin_layout Subsection
Parameter learning
\end_layout

\begin_layout Standard
The neural network is trained to optimize parameters using a Stochastic
 Gradient Descent (SGD) in order to minimize the squared error between the
 estimate and the original source 
\begin_inset Formula $y_{n}$
\end_inset

.
 Formally
\begin_inset Formula 
\[
L_{sq}=\sum_{i=1}^{N}\left\Vert \widetilde{y}_{n}-y_{n}\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Section
Training
\end_layout

\begin_layout Subsection
Dataset
\end_layout

\begin_layout Standard
The authors used the 
\series bold
Demixing Secrets Dataset 100
\series default
 (DSD100) for training and testing.
 This dataset consists of 100 professionally produced full track songs and
 is designed to evaluate signal source separation methods from music recordings.
 The dataset contains separate tracks for drums, bass, vocals and other
 instruments for each song in the set, present as stereo wav files with
 a sample rate of 44.1 KHz.
 The four source tracks are mixed using a professional Digital Audio Workstation
 to create the mixture track for each song.
 The dataset is divided into a dev set, used for training the network and
 a test set, which is used for testing the network.
 Both of these subsets consist of 50 songs each.
\end_layout

\begin_layout Subsection
Training
\end_layout

\begin_layout Standard
During the training phase, the input mixture and the individual sources
 comprising the mixture were split into 20 seconds segments, and the short
 time Fourier transform (STFT) for each of these segments was computed.
 A Hanning window of length 1024 samples was used, which, at a sampling
 rate of 44.1 KHz corresponds to 23 milliseconds (ms), and a hopsize of 256
 samples (5.8 ms), leading to an overlap of 75% across frames.
 The frames generated from this procedure were grouped into batches of 
\begin_inset Formula $T$
\end_inset

 frames, representing the maximum time context that the network tries to
 model.
 The batches were fed to the model for training, with 30 batches being fed
 at each round.
 Thus, each batch consists of 
\begin_inset Formula $T$
\end_inset

 frames of 513 frequency bins.
 A complete pass over the entire set is considered as one training epoch
 and the network is trained for 30 epochs.
\end_layout

\begin_layout Section
Evaluation
\end_layout

\begin_layout Standard
For evaluation, the following measures were used: Source to Distortion Ratio
 (SDR), Source to Interference Ratio (SIR), Source to Artifacts Ratio(SAR),
 and Image to Spatial distortion Ratio (ISR).
 These measures are averaged for overlapping 30 second frames of each song
 in both the development and the test set.
\end_layout

\begin_layout Section
Result Demonstration
\end_layout

\begin_layout Section
Limitations and Proposed Ways to Alleviate Them
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

P.
 Chandna, M.
 Miron, J.
 Janer, E.
 GÂ´omez.
 Monoaural audio source separation using deep convolutional neural networks.
 In 13th International Conference on Latent Variable Analysis and Signal
 Separation (LVAICA2017), 02/2017 2017
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

Durrieu, J., Ozerov, A., and F Ìevotte, C.
 (2009).
 Main instrument separation from stereophonic audio signals using a source/filte
r model.
 17th European Signal Processing Conference.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

Rafii, Z., Liutkus, A., and Pardo, B.
 (2014).
 REPET for Background/Foreground Separation in Audio, pages 395â411.
 Springer Berlin Heidelberg, Berlin, Heidelberg.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"

\end_inset

Nugraha, A.
 A., Liutkus, A., and Vincent, E.
 (2016).
 Multichannel audio source separation with deep neural networks.
 Technical report.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-5"

\end_inset

Huang, P.-S., Kim, M., Hasegawa-Johnson, M., and Smaragdis, P.
 (2014).
 Deep Learning for Monaural Speech Separation.
 Acoustics, Speech and Signal Processing (ICASSP), pages 1562â1566.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-6"

\end_inset

Uhlich, S., Giron, F., and Mitsufuji, Y.
 (2015).
 Deep neural network basedinstrument extraction from music.
 2015 IEEE International Conference on Acoustics, Speech and Signal Processing
 (ICASSP), pages 2135â2139.
 IEEE.
\end_layout

\end_body
\end_document
